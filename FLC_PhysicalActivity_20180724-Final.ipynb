{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facility List Coder - Physical Activity, version 24/07/2018  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Facility List coder (FLC) is an open source tool that allow efficiently combine GIS analysis with standard data techniques. Besides the data management tools, FLC code retrieves GIS information on facilities location using two open source datasets: Google Maps and Open Street Maps. \n",
    "\n",
    "FLC is built upon two main requirements. \n",
    "- First, researchers will need to provide a specific data set for the specific location of the reference point (e.g. school, university, among others). We call them as location of interest (LI). \n",
    "- Second in order to classify the results obtained from the spatial query on the traditional GIS engines, researchers will need to define a set of key words or metadata that allow the algorithm classify the facilities. Based on the literature, we have developed a multi-language key words list based on the European context that allows to classify each facility within a pre-defined category. These categories could be modified in order to fulfill specific needs of researchers related to geographical location, languages or research questions. This pre-defined key word offers an important innovation for the research on food community environment in the European context. The empirical studies for Europe often use categories created for United States, which might miss-estimate the particularities of European food traditions. Nonetheless, this list will be easily extended depending on the needs of the researchers. We called this dimension as key words by categories (KW).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FLC code retrieves GIS information on facilities location using two open source datasets: Google Maps and Open Street Maps. All the code was developed on Python 3.6.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Defining functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to create functions that will be use during the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to save results \n",
    "## \n",
    "def save_result(res,prints):\n",
    "    for place in res.places:\n",
    "        place.get_details()\n",
    "        if prints==1:\n",
    "            print(place.name.encode('utf-8'),place.types,place.place_id)\n",
    "        else:\n",
    "            if place.place_id not in places_flc:\n",
    "                raw={'place_name':place.name,'place_lat':float(place.geo_location['lat']),'place_lng':float(place.geo_location['lng']),'place_address':place.formatted_address,'google_id':place.place_id,'place_types':place.types,'place_web':place.url}\n",
    "                places_flc[place.place_id]=raw\n",
    "                    \n",
    "\n",
    "## Read \n",
    "## This functions was made to test the results\n",
    "def read_result(res):\n",
    "    for place in res.places:\n",
    "        place.get_details()\n",
    "        print(place.name.encode('utf-8'),place.types,place.place_id)\n",
    "    \n",
    "                \n",
    "### Save Results\n",
    "## Save Results\n",
    "def sav_final(place,cat,final_ds):\n",
    "    ## Save Final Data Set\n",
    "    raw=places_flc_cleaned[place]\n",
    "    raw['categ']=cat\n",
    "    final_ds.append(raw)\n",
    "    ## Delete the dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading packages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we need to load the needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "###########################\n",
    "###### Libraries and Initial SetUp\n",
    "###########################\n",
    "\n",
    "### My Data key\n",
    "YOUR_API_KEY = 'AIzaSyDfJ-C6nJJaEIiwsWuh3JMmD6rcXijwKqA'\n",
    "\n",
    "## --- Google maps\n",
    "import googlemaps\n",
    "gmaps = googlemaps.Client(key=YOUR_API_KEY)\n",
    "from datetime import datetime\n",
    "\n",
    "# ----- Google Place\n",
    "from googleplaces import GooglePlaces, types, lang\n",
    "import google_streetview.api\n",
    "google_places = GooglePlaces(YOUR_API_KEY)\n",
    "\n",
    "# ----- Download the data\n",
    "import urllib.request # Download the files\n",
    "import os, sys ## create a new \n",
    "import pandas as pd\n",
    "from xlsxwriter.utility import xl_rowcol_to_cell\n",
    "\n",
    "# ------ from openpyxl import load_workbook\n",
    "from openpyxl import load_workbook\n",
    "import fiona\n",
    "\n",
    "# ------ Import Regex\n",
    "import re\n",
    "\n",
    "# ----- Load information\n",
    "import json\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input 1:  Key Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we load the key words that will be used for the classification of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key words for the Catalan context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of the key words are a detailed description of the economic activities in a specific region. This document brings a details description of the different types of establishments present in the region. \n",
    "\n",
    "All the keywords were translated in English, Spanish and Catalan. Everything is saved at: **Palabras_clave_v0_120917.xlsx**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_keywords=\"/home/datascience/Documents/FLC_Andreu/01_DataBase_Georeferencion/Palabras_clave_v0_120917.xlsx\"\n",
    "wb = load_workbook(filename = data_keywords)\n",
    "palabras = wb['Palabras_Claves_Google2']\n",
    "\n",
    "### Create the dictionary\n",
    "categ_est={}\n",
    "keywords=[]\n",
    "for row in range(2,32):\n",
    "    cat=str(palabras.cell(row=row, column=1).value.encode('utf-8')).lower().strip()\n",
    "    try:\n",
    "        categ_est[cat]=[]\n",
    "    except:\n",
    "        pass;\n",
    "\n",
    "#### Now the key words\n",
    "for row in range(2,32):\n",
    "    cat=str(palabras.cell(row=row, column=1).value.encode('utf-8')).lower().strip()\n",
    "    for col in [2,3,4]:\n",
    "        if palabras.cell(row=row, column=col).value is not None:\n",
    "            word=str(palabras.cell(row=row, column=col).value.encode('utf-8')).lower().strip()\n",
    "            if word not in categ_est[cat]:\n",
    "                categ_est[cat].append(str(word))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Excluded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to optimize the query over the google maps we restrict the search to particular set of places types ([here the list of accepted types in Google](https://developers.google.com/places/supported_types)).\n",
    "\n",
    "In order to exclude/include a given category you only have to change: **google_types.xlsx**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amusement_park', 'bowling_alley', 'campground', 'gym', 'park']\n"
     ]
    }
   ],
   "source": [
    "data_types=\"/home/datascience/Documents/FLC_Andreu/01_DataBase_Georeferencion/google_types.xlsx\"\n",
    "wb = load_workbook(filename = data_types)\n",
    "palabras = wb['Sheet1']\n",
    "types_accepted=[]\n",
    "types_excluded=[]\n",
    "for row in range(2,98):\n",
    "    selected=int(palabras.cell(row=row, column=2).value)\n",
    "    if selected==0:\n",
    "        types_accepted.append(str(palabras.cell(row=row, column=1).value))\n",
    "    if selected==1:\n",
    "        types_excluded.append(str(palabras.cell(row=row, column=1).value))\n",
    "\n",
    "print (types_accepted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input 2: Defining Location  of interest (LI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we load the location of the point of interest. For the validation procedure, we include the Grids. \n",
    "\n",
    "Note that you need a shapefile with the centroids. It can be generalized even more using any type of argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids=\"/home/datascience/Documents/FLC_Andreu/01_DataBase_Georeferencion/Colegios_shp/Colegios.shp\"\n",
    "grids_data={}\n",
    "with fiona.open(grids,'r') as shp:\n",
    "    for feat in shp:\n",
    "        grid_id = feat['properties']['1']\n",
    "        lng,lat=feat['geometry']['coordinates']\n",
    "        grids_data[grid_id]={'lat':lat,'lng':lng}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facility List Coder in action: Building the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we run the spatial query in Google maps based on the point of interest. Technically, the FLC will get all the location within an specific folder, then we will classify then using the key words.\n",
    "\n",
    "The first step is defining the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Information\n",
    "places_flc={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Query Strategy 1: Using only Google types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This strategy will get all the places within a buffer that belongs to a specific type (look above to check the list of types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Query Strategy 2: Using only Google types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of 301 (grid=2)\n",
      "2 of 301 (grid=4)\n",
      "3 of 301 (grid=5)\n",
      "4 of 301 (grid=6)\n",
      "5 of 301 (grid=7)\n",
      "6 of 301 (grid=8)\n",
      "7 of 301 (grid=9)\n",
      "8 of 301 (grid=10)\n",
      "9 of 301 (grid=11)\n",
      "10 of 301 (grid=12)\n",
      "11 of 301 (grid=13)\n",
      "12 of 301 (grid=14)\n",
      "13 of 301 (grid=15)\n",
      "14 of 301 (grid=16)\n",
      "15 of 301 (grid=17)\n",
      "16 of 301 (grid=18)\n",
      "17 of 301 (grid=19)\n",
      "18 of 301 (grid=20)\n",
      "19 of 301 (grid=21)\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "\n",
    "### To Print results\n",
    "prints=0\n",
    "\n",
    "#for grid in [755]:\n",
    "for grid in grids_data:\n",
    "    print(\"%d of 301 (grid=%d)\" %(i,grid))\n",
    "    i+=1\n",
    "    ### First Step Search\n",
    "    for typ in types_accepted:\n",
    "   #for typ in ['restaurant']:\n",
    "        ### First 20 results\n",
    "        res1=google_places.nearby_search(lat_lng=grids_data[grid],radius=100,types=[typ])\n",
    "        save_result(res1,prints)\n",
    "        # For more than 20 results\n",
    "        try:\n",
    "            res2 = google_places.nearby_search(pagetoken=res1.next_page_token)\n",
    "            save_result(res2,prints)\n",
    "            try:\n",
    "                res3 = google_places.nearby_search(pagetoken=res2.next_page_token)\n",
    "                save_result(res3,prints)\n",
    "                try:\n",
    "                    res4 = google_places.nearby_search(pagetoken=res3.next_page_token)\n",
    "                    save_result(res4,prints)\n",
    "                except:\n",
    "                    pass  \n",
    "            except:\n",
    "                pass    \n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is complementary to the last one, but it will search by keywords instead of type. This part will search at any information gathered by google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of 301 (grid=2)\n",
      "2 of 301 (grid=4)\n",
      "3 of 301 (grid=5)\n",
      "4 of 301 (grid=6)\n",
      "5 of 301 (grid=7)\n",
      "6 of 301 (grid=8)\n",
      "7 of 301 (grid=9)\n",
      "8 of 301 (grid=10)\n",
      "9 of 301 (grid=11)\n",
      "10 of 301 (grid=12)\n",
      "11 of 301 (grid=13)\n",
      "12 of 301 (grid=14)\n",
      "13 of 301 (grid=15)\n",
      "14 of 301 (grid=16)\n",
      "15 of 301 (grid=17)\n",
      "16 of 301 (grid=18)\n",
      "17 of 301 (grid=19)\n",
      "18 of 301 (grid=20)\n",
      "19 of 301 (grid=21)\n"
     ]
    }
   ],
   "source": [
    "#types=types_accepted+['supermarket']\n",
    "\n",
    "### To Print results\n",
    "prints=0\n",
    "\n",
    "i=1\n",
    "for grid in grids_data:\n",
    "    print(\"%d of 301 (grid=%d)\" %(i,grid))\n",
    "    i+=1\n",
    "#for grid in [1020]:\n",
    "    ### First Step Search\n",
    "    for typ in types_accepted:\n",
    "        #print typ\n",
    "        ### First 20 results\n",
    "        res1=google_places.nearby_search(lat_lng=grids_data[grid],radius=100,keyword=typ)\n",
    "        # For more than 20 results\n",
    "        save_result(res1,prints)\n",
    "        try:\n",
    "            res2 = google_places.nearby_search(pagetoken=res1.next_page_token)\n",
    "            save_result(res2,prints)\n",
    "            try:\n",
    "                res3 = google_places.nearby_search(pagetoken=res2.next_page_token)\n",
    "                save_result(res3,prints)\n",
    "                try:\n",
    "                    res4 = google_places.nearby_search(pagetoken=res3.next_page_token)\n",
    "                    save_result(res4,prints)\n",
    "                except:\n",
    "                    pass    \n",
    "            except:\n",
    "                pass\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(places_flc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facility List Coder in action: Classifying the places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, once all the places are gathered using the two strategies, we now need to clean them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will delete those establishments that will be excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First data\n",
    "places_flc_cleaned={}\n",
    "\n",
    "# Excluded Types\n",
    "excluded_types=['beauty_salon','finance','pharmacy','electrician','church','parking']\n",
    "# Excluded Names (part of)\n",
    "excluded_names=['parking']\n",
    "\n",
    "#### First Step ---> Those in the excluded categories\n",
    "for i in places_flc:\n",
    "    ### Check for those with a excluded\n",
    "    name=places_flc[i]['place_name'].lower().encode('utf-8').strip().split()\n",
    "    if any(x in places_flc[i]['place_types'] for x in types_excluded):\n",
    "        pass\n",
    "    if any(x in name for x in excluded_names):\n",
    "        pass\n",
    "    else:\n",
    "        places_flc_cleaned[i]=places_flc[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save RawData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the entire data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "### Save\n",
    "json = json.dumps(places_flc_cleaned)\n",
    "f = open(\"/home/datascience/Documents/FLC_Andreu/Results/flc_rawresults_grids_13022018.json\",\"w\")\n",
    "f.write(json)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
